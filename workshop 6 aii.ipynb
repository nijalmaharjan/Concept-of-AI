{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kMt3kKzwWPC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def logistic_function(x):\n",
        "    \"\"\"\n",
        "    Computes the logistic function applied to any value of x.\n",
        "    Arguments:\n",
        "        x: scalar or numpy array of any size.\n",
        "    Returns:\n",
        "        y: logistic function applied to x.\n",
        "    \"\"\"\n",
        "    y = 1 / (1 + np.exp(-x))\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def log_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Computes log loss for true target value y = {0 or 1} and predicted target value y' in {0-1}.\n",
        "    \"\"\"\n",
        "    # Ensure y_pred is clipped to avoid log(0)\n",
        "    y_pred = np.clip(y_pred, 1e-10, 1 - 1e-10)\n",
        "    loss = -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "    return loss"
      ],
      "metadata": {
        "id": "lXJrawoPw6Jy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cost_function(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Computes log loss for inputs true value (0 or 1) and predicted value (between 0 and 1)\n",
        "    \"\"\"\n",
        "    assert len(y_true) == len(y_pred), \"Length mismatch\"\n",
        "    n = len(y_true)\n",
        "    loss_vec = log_loss(y_true, y_pred)\n",
        "    cost = np.sum(loss_vec) / n\n",
        "    return cost"
      ],
      "metadata": {
        "id": "jKw_tQhSw9eC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def costfunction_logreg(X, y, w, b):\n",
        "    \"\"\"\n",
        "    Computes the cost function, given data and model parameters.\n",
        "    \"\"\"\n",
        "    n, d = X.shape\n",
        "    assert len(y) == n, \"Number of observations mismatch\"\n",
        "    assert len(w) == d, \"Number of features mismatch\"\n",
        "\n",
        "    # Compute z using np.dot\n",
        "    z = np.dot(X, w) + b  # Matrix-vector multiplication and adding bias\n",
        "\n",
        "    # Compute predictions using logistic function\n",
        "    y_pred = logistic_function(z)\n",
        "\n",
        "    # Compute the cost\n",
        "    cost = cost_function(y, y_pred)\n",
        "    return cost"
      ],
      "metadata": {
        "id": "KrXNgK6-xBgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def costfunction_logreg(X, y, w, b):\n",
        "    z = np.dot(X, w) + b\n",
        "    y_pred = logistic_function(z)\n",
        "    return cost_function(y, y_pred)\n",
        "def compute_gradient(X, y, w, b):\n",
        "    n, d = X.shape\n",
        "    z = np.dot(X, w) + b\n",
        "    y_pred = logistic_function(z)\n",
        "\n",
        "    error = y_pred - y\n",
        "    grad_w = np.dot(X.T, error) / n\n",
        "    grad_b = np.sum(error) / n\n",
        "    return grad_w, grad_b\n",
        "X = np.array([[10,20],[-10,10]])\n",
        "y = np.array([1,0])\n",
        "w = np.array([0.5,1.5])\n",
        "b = 1\n",
        "grad_w, grad_b = compute_gradient(X,y,w,b)\n",
        "print(\"Gradient test passed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxVgInERxF25",
        "outputId": "f6fc0c3a-4dfe-41fc-aeb7-06f7e7217994"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient test passed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient(X, y, w, b):\n",
        "    \"\"\"\n",
        "    Computes gradients of the cost function with respect to model parameters.\n",
        "    \"\"\"\n",
        "    n, d = X.shape\n",
        "    assert len(y) == n, f\"Expected y to have {n} elements\"\n",
        "    assert len(w) == d, f\"Expected w to have {d} elements\"\n",
        "\n",
        "    # Compute predictions\n",
        "    z = np.dot(X, w) + b\n",
        "    y_pred = logistic_function(z)\n",
        "\n",
        "    # Compute gradients\n",
        "    error = y_pred - y\n",
        "    grad_w = np.dot(X.T, error) / n  # Gradient w.r.t weights\n",
        "    grad_b = np.sum(error) / n       # Gradient w.r.t bias\n",
        "\n",
        "    return grad_w, grad_b"
      ],
      "metadata": {
        "id": "IH8LMDzRxUbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, y, w, b, alpha, n_iter, show_cost=False, show_params=True):\n",
        "    \"\"\"\n",
        "    Implements batch gradient descent to optimize logistic regression parameters.\n",
        "    \"\"\"\n",
        "    n, d = X.shape\n",
        "    cost_history = []\n",
        "    params_history = []\n",
        "\n",
        "    for i in range(n_iter):\n",
        "        # Compute gradients\n",
        "        grad_w, grad_b = compute_gradient(X, y, w, b)\n",
        "\n",
        "        # Update weights and bias\n",
        "        w -= alpha * grad_w\n",
        "        b -= alpha * grad_b\n",
        "\n",
        "        # Compute cost\n",
        "        cost = costfunction_logreg(X, y, w, b)\n",
        "\n",
        "        # Store history\n",
        "        cost_history.append(cost)\n",
        "        params_history.append((w.copy(), b))\n",
        "\n",
        "        # Optional printing\n",
        "        if show_cost and (i % 100 == 0 or i == n_iter - 1):\n",
        "            print(f\"Iteration {i}: Cost = {cost:.6f}\")\n",
        "        if show_params and (i % 100 == 0 or i == n_iter - 1):\n",
        "            print(f\"Iteration {i}: w = {w}, b = {b:.6f}\")\n",
        "\n",
        "    return w, b, cost_history, params_history"
      ],
      "metadata": {
        "id": "oPni9G-9xaGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction(X, w, b, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Predicts binary outcomes for given input features.\n",
        "    \"\"\"\n",
        "    # Compute predicted probabilities\n",
        "    z = np.dot(X, w) + b\n",
        "    y_test_prob = logistic_function(z)\n",
        "\n",
        "    # Classify based on threshold\n",
        "    y_pred = (y_test_prob >= threshold).astype(int)\n",
        "\n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "Hl-olnaaxcdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_classification(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Computes confusion matrix, precision, recall, and F1-score.\n",
        "    \"\"\"\n",
        "    # Initialize confusion matrix components\n",
        "    TP = np.sum((y_true == 1) & (y_pred == 1))  # True Positives\n",
        "    TN = np.sum((y_true == 0) & (y_pred == 0))  # True Negatives\n",
        "    FP = np.sum((y_true == 0) & (y_pred == 1))  # False Positives\n",
        "    FN = np.sum((y_true == 1) & (y_pred == 0))  # False Negatives\n",
        "\n",
        "    # Confusion matrix\n",
        "    confusion_matrix = np.array([[TN, FP],\n",
        "                                 [FN, TP]])\n",
        "\n",
        "    # Precision, recall, and F1-score\n",
        "    precision = TP / (TP + FP) if (TP + FP) > 0.0 else 0.0\n",
        "    recall = TP / (TP + FN) if (TP + FN) > 0.0 else 0.0\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
        "\n",
        "    # Metrics dictionary\n",
        "    metrics = {\n",
        "        \"confusion_matrix\": confusion_matrix,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1_score\": f1_score\n",
        "    }\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "zqwQkz6OxfGb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}